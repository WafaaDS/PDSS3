{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2A9WlOg2Ymzy"
      },
      "source": [
        "# Exersise 1:\n",
        "Play with the code you have worked out in the lecture. For example, change the input data and the output data and try to get a good understanding of what is happening and how the neural network adjusts the weights. Then, create a Neural Network with a weight matrix of dimension (6,1).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VV3JHLpwYjE4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "250e0918-8eb0-4aa0-abc1-e080c36a7610"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Data\n",
            "Input row 0: [0 0 1 0 0 1]\n",
            "Input row 1: [1 1 0 1 1 0]\n",
            "Input row 2: [1 0 0 1 0 1]\n",
            "Input row 3: [0 1 1 1 0 0]\n",
            "\n",
            "Target Data\n",
            "Target neuron 0: 0\n",
            "Target neuron 1: 1\n",
            "Target neuron 2: 1\n",
            "Target neuron 3: 0\n",
            "\n",
            "Hidden Layer Weights\n",
            "Weight of input-column 0: 4.175578139060073\n",
            "Weight of input-column 1: -0.7145980862917435\n",
            "Weight of input-column 2: -5.510426493856767\n",
            "Weight of input-column 3: 0.9803943999415218\n",
            "Weight of input-column 4: 1.1040695834352683\n",
            "Weight of input-column 5: 0.1708031488771556\n",
            "\n",
            "Predicted Output After Training\n",
            "Predicted output 0: 0.004775004892136923\n",
            "Predicted output 1: 0.9961097808538929\n",
            "Predicted output 2: 0.9951635484650447\n",
            "Predicted output 3: 0.005248353404270549\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# INPUT LAYER\n",
        "X = np.array([\n",
        "    [0, 0, 1, 0, 0, 1],\n",
        "    [1, 1, 0, 1, 1, 0],\n",
        "    [1, 0, 0, 1, 0, 1],\n",
        "    [0, 1, 1, 1, 0, 0]\n",
        "])\n",
        "\n",
        "# OUTPUT LAYER\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# HIDDEN LAYER\n",
        "# Sigmoid function\n",
        "def nonlin(x, deriv=False):\n",
        "    if deriv:\n",
        "        return x * (1 - x)\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Seed random numbers to make calculation deterministic\n",
        "np.random.seed(1)\n",
        "\n",
        "# Initialize weights randomly with mean 0\n",
        "weights = 2 * np.random.random((6, 1)) - 1\n",
        "\n",
        "\n",
        "for iter in range(10000):\n",
        "    # Forward propagation\n",
        "    l0 = X\n",
        "    pout = nonlin(np.dot(l0, weights))\n",
        "\n",
        "    # Calculate the error\n",
        "    pout_error = y - pout\n",
        "\n",
        "    # Backpropagation: Calculate delta\n",
        "    pout_delta = pout_error * nonlin(pout, deriv=True)\n",
        "\n",
        "    # Update weights\n",
        "    weights += np.dot(l0.T, pout_delta)\n",
        "\n",
        "# Print Input Data\n",
        "print(\"Input Data\")\n",
        "for i, row in enumerate(X):\n",
        "    print(f\"Input row {i}: {row}\")\n",
        "\n",
        "# Print Target Data\n",
        "print(\"\\nTarget Data\")\n",
        "for i, target in enumerate(y):\n",
        "    print(f\"Target neuron {i}: {target[0]}\")\n",
        "\n",
        "# Print Weights\n",
        "print(\"\\nHidden Layer Weights\")\n",
        "# This assumes weights are a 2D array; flatten if required for printing\n",
        "for i, weight in enumerate(weights.flatten()):\n",
        "    print(f\"Weight of input-column {i}: {weight}\")\n",
        "\n",
        "# Print Predicted Output After Training\n",
        "print(\"\\nPredicted Output After Training\")\n",
        "for i, output in enumerate(pout):\n",
        "    print(f\"Predicted output {i}: {output[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-RwhWW3ZdX5"
      },
      "source": [
        "# Exersise 2:\n",
        "What happens if you reduce the number of iterations of your Forward Propagation algorithm?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "dAf2eXIuZrk9"
      },
      "outputs": [],
      "source": [
        "# Since we train for fewer iterations, the predicted output values will be further from the actual target values.\n",
        "# The error might be high therefire shows the importance of training for a fair number of iterations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ImwfjN3ZsHm"
      },
      "source": [
        "# Exersise 3:\n",
        "Below is a function written in Python. The function takes an input (x) and transforms it into an output (y). Can you explain what kind of transformation it does? Can you think of why it might be useful?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXA5MN5BdES-",
        "outputId": "52934639-438c-4d9e-f80a-1cae01e9bfcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.01521943 0.0413707  0.11245721 0.83095266]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\" applies softmax to an input x\"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    # Subtracting max(x) keeps the numbers small and prevents very large values.\n",
        "    return e_x / e_x.sum()\n",
        "\n",
        "x = np.array([1, 2, 3, 5])\n",
        "y = softmax(x)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Softmax** is a function that converts a vector of real numbers into a probability distribution. It ensures that all output values sum to **1**, making them interpretable as probabilities.\n",
        "\n",
        "It is useful to highlight the most likely category as the largest input value receives the highest probability.\n",
        "\n",
        "The example above means that the last value of 5 has the highest probability, making it **the most likely category**.\n"
      ],
      "metadata": {
        "id": "eBUiSXBGkW6c"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}